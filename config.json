{
    "encoder_layers": 6,
    "decoder_layers": 6,
    "embedding_dim": 512,
    "num_heads": 8,
    "dropout_rate": 0.1,
    "max_length": 512,
    "latent_dim": 256,
    "batch_size": 64,
    "learning_rate": 0.0001,
    "attention_type": "MHA"
}